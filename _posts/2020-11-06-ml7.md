---
layout: post
title: "[ML] 7 Softmax Regression"
date: 2020-11-06
categories: ETC
tags: [MahineLearning,Development]
---

## Multinomial classification
A,B,C 정도의 등급으로 나타내는 방법.


## Softmax Regreesion
다음과 같은 예측 값이 있다고 하자.
A : 2.0 / B: 1.0 / C : 0.1 
예측 값 중 2.0 이 가장 크기 때문에 A라고 예측할 수 있다. 이렇게 예측하기 위해 다음과 같은 과정을 거친다.

step1 ) sigmoid 함수를 이용하여 0과 1사이의 값으로 나타냄

step2 ) Softmax 함수를 사용하여 세개의 값의 합이 1.0 이 되도록 만든다. 그러면 각각 0.7,0.2,0.1 값을 가지게 된다.

step3 ) 원 핫 인코딩을 이용하여 가장 큰값을 1로 , 나머지 값을 0으로 만들어 주어 명확하게 나타내준다.즉 1.0 , 0.0,0.0 이 된다.

이렇게 가설을 완성했다!



## Softmax Classifier 실습
다음 코드는 x data 가 어떤 class에 속하는지 먼저 데이터를 주어 학습시킨 후, 입력한 데이터들이 어떤 class 에 속하는지 예측하는 코드이다. 예를들어 x_data 의 첫번째 값인 [1,2,1,1]은 [0,0,1] 즉 2 class 에 속한다고 할 수 있다. (0은 [000], 1은[010], 2는[001])

```python
import tensorflow as tf
tf.set_random_seed(777)  # for reproducibility

x_data = [[1, 2, 1, 1],
          [2, 1, 3, 2],
          [3, 1, 3, 4],
          [4, 1, 5, 5],
          [1, 7, 5, 5],
          [1, 2, 5, 6],
          [1, 6, 6, 6],
          [1, 7, 7, 7]]
y_data = [[0, 0, 1],
          [0, 0, 1],
          [0, 0, 1],
          [0, 1, 0],
          [0, 1, 0],
          [0, 1, 0],
          [1, 0, 0],
          [1, 0, 0]]

X = tf.placeholder("float", [None, 4]) # x data가 4개라서
Y = tf.placeholder("float", [None, 3]) # y data가 3자리로 값을 표현하고 있기 때문에 3. 
nb_classes = 3 # 3개의 class를 이용하고있다 (0,1,2) 혹은 abc.

W = tf.Variable(tf.random_normal([4, nb_classes]),  name='weight') # x data가 4개
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')


hypothesis = tf.nn.softmax(tf.matmul(X, W) + b)

# Cross entropy cost/loss
cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

# Launch graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
            _, cost_val = sess.run([optimizer, cost], feed_dict={X: x_data, Y: y_data})

            if step % 200 == 0:
                print(step, cost_val)

    print('--------------')
    # Testing & One-hot encoding
    a = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9]]})
    print(a, sess.run(tf.argmax(a, 1)))

    print('--------------')
    b = sess.run(hypothesis, feed_dict={X: [[1, 3, 4, 3]]})
    print(b, sess.run(tf.argmax(b, 1)))

    print('--------------')
    c = sess.run(hypothesis, feed_dict={X: [[1, 1, 0, 1]]})
    print(c, sess.run(tf.argmax(c, 1)))

    print('--------------')
    all = sess.run(hypothesis, feed_dict={X: [[1, 11, 7, 9], [1, 3, 4, 3], [1, 1, 0, 1]]})
    print(all, sess.run(tf.argmax(all, 1)))
```

__결과 !__

```python
--------------
[[1.3890496e-03 9.9860185e-01 9.0612912e-06]] [1]
# 가운데꺼가 젤 높으니까 1번 class 출력
--------------
[[0.93119204 0.06290206 0.0059059 ]] [0]
# 맨 왼쪽이 젤 높으니까 0번 class 
--------------
[[1.2732767e-08 3.3411290e-04 9.9966586e-01]] [2]
# 맨 오른쪽이 젤 높아서 2번 class
--------------
[[1.3890496e-03 9.9860185e-01 9.0612912e-06]
 [9.3119204e-01 6.2902056e-02 5.9058950e-03]
 [1.2732767e-08 3.3411290e-04 9.9966586e-01]] 
 [1 0 2]
# 한꺼번에 물어본 경우.
```

## TensorFlow로 Fancy Softmax Classification의 구현하기
어떤 특징들 (x) 에 따라 종(y)을 예측하는 코드.

```python
import tensorflow as tf
import numpy as np
tf.set_random_seed(777)  # for reproducibility

# Predicting animal type based on various features
xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)
x_data = xy[:, 0:-1]
y_data = xy[:, [-1]]


nb_classes = 7  # 0 ~ 6

X = tf.placeholder(tf.float32, [None, 16]) 
Y = tf.placeholder(tf.int32, [None, 1]) 

Y_one_hot = tf.one_hot(Y, nb_classes)  # one hot
Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])

W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')
b = tf.Variable(tf.random_normal([nb_classes]), name='bias')

logits = tf.matmul(X, W) + b
hypothesis = tf.nn.softmax(logits)

cost_i = tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=Y_one_hot)
cost = tf.reduce_mean(cost_i)

optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)

prediction = tf.argmax(hypothesis, 1)
correct_prediction = tf.equal(prediction, tf.argmax(Y_one_hot, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# Launch graph
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(2001):
        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})
                                        
        if step % 100 == 0:
            print("Step: {:5}\tCost: {:.3f}\tAcc: {:.2%}".format(step, cost_val, acc_val))

    # Let's see if we can predict
    pred = sess.run(prediction, feed_dict={X: x_data})
    # y_data: (N,1) = flatten => (N, ) matches pred.shape
    for p, y in zip(pred, y_data.flatten()):
        print("[{}] Prediction: {} True Y: {}".format(p == int(y), p, int(y)))
```

__결과 !__

예측한 후 예측한 값과 실제 값을 비교하여 예측한 값이 맞는지 확인.

```python
Step:     0     Cost: 5.480     Acc: 37.62%
Step:   100     Cost: 0.806     Acc: 79.21%
...
Step:  1900     Cost: 0.057     Acc: 100.00%
Step:  2000     Cost: 0.054     Acc: 100.00%
[True] Prediction: 0 True Y: 0
[True] Prediction: 0 True Y: 0
...
[True] Prediction: 6 True Y: 6
[True] Prediction: 1 True Y: 1
```