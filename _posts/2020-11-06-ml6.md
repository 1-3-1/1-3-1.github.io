---
layout: post
title: "[ML] 6 Logistic Classification"
date: 2020-11-06
categories: ETC
tags: [MahineLearning,Development]
---

# Logistic Classification이란
둘 중에 하나를 고르는 것. 예를들면 스팸인지 아닌지. 또는 페이스북 피드. 선별해서 재밌고 좋은거만 보여줌. 고로 결과가 0과 1 중 하나로 나타내어져야한다

## Logistic Hypothesis
logistic hypothesis는 다음과 같이 나타내어진다
`H(X) = 1/ 1+e^(-W^T*X)`

이 함수는 무조건 0 아니면 1로 수렴하게 된다.

## Cost function
C( H(x),y ) = ylog(H(x)) - (1-y)log(1-H(x))
y가 1일 때는 뒤에 있는 항이 없어지고 y가 0일 때는 앞에 있는 항이 없어진다

## 실습
```python
import tensorflow as tf

x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]] # x값이 2개
y_data = [[0],[0],[0],[1],[1],[1]]



X = tf.placeholder(tf.float32, shape=[None, 2])
# 들어오는 x값의 갯수
Y = tf.placeholder(tf.float32, shape=[None, 1])
# 나가는 y값의 갯수


W = tf.Variable(tf.random_normal([2,1]), name = 'weight')
# x값이 2개, y값이 1개

b = tf.Variable(tf.random_normal([1]),name='bias')


# logistic hypothesis
hypothesis = tf.sigmoid(tf.matmul(X,W)+b)


# cost function
cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
#0.5보다 크면 1 (true) 작으면 0 (false)


accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype=tf.float32))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(10001):
        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
        if step % 200 == 0:
            print(step, cost_val)

    h,c,a = sess.run([hypothesis ,predicted,accuracy],feed_dict={X: x_data, Y: y_data})
    print("\nHypothesis: ", h,"\nCorrect (Y):", c, "\nAccuracy: ",a)
# hypothesis는 계산한 값. 0.8 , 0.9 ...
# predicted는 1, 0 으로 나오는 값. 19번째줄
# accuracy는 예측한 값과 정답 값을 비교해서 맞는지 틀린지 출력
```

## csv 파일을 불러와 예측하는 실습
예를 들어 당뇨에 대해 당뇨를 결정하는 변수가 x1 ~ x3 까지있고, 당뇨병의 유무를 y로 나타낸다고 할때,( 1이면 걸린거 0이면 안걸린거) 다음과 같은 csv 자료가 있다고 가정하고 실습해보자.

doctor.csv
x1 | x2 | x3 | y
123| 142|412|0
13| 422|423|0
83| 2142|532|1

```python
import tensorflow as tf

xy = np.loadtxt('doctor.csv', delimiter=',', dtype=np.float32)
x_data = xy[:,0:-1] # x1,x2,x3 속성만 가져옴
y_data = xy[:,[-1]] # y속성만 가져옴

X = tf.placeholder(tf.float32, shape=[None, 3])
# 들어오는 x값의 갯수
Y = tf.placeholder(tf.float32, shape=[None, 1])
# 나가는 y값의 갯수


W = tf.Variable(tf.random_normal([3,1]), name = 'weight')
# x값이 3개, y값이 1개

b = tf.Variable(tf.random_normal([1]),name='bias')


# logistic hypothesis
hypothesis = tf.sigmoid(tf.matmul(X,W)+b)


# cost function
cost = -tf.reduce_mean(Y* tf.log(hypothesis) + (1-Y) * tf.log(1-hypothesis))

train = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)

predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)
#0.5보다 크면 1 (true) 작으면 0 (false)


accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y),dtype=tf.float32))

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())

    for step in range(10001):
        cost_val, _ = sess.run([cost, train], feed_dict={X: x_data, Y: y_data})
        if step % 200 == 0:
            print(step, cost_val)

    h,c,a = sess.run([hypothesis ,predicted,accuracy],feed_dict={X: x_data, Y: y_data})
    print("\nHypothesis: ", h,"\nCorrect (Y):", c, "\nAccuracy: ",a)
# hypothesis는 계산한 값. 0.8 , 0.9 ...
# predicted는 1, 0 으로 나오는 값. 19번째줄
# accuracy는 예측한 값과 정답 값을 비교해서 맞는지 틀린지 출력
```
